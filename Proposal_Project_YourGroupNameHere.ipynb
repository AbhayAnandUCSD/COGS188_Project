{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Project Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "You have the choice of doing either (1) an AI solve a problem style project or (2) run a Special Topics class on a topic of your choice.  If you want to do (2) you should fill out the _other_ proposal for that. This is the proposal description for (1).\n",
    "\n",
    "You will design and execute a machine learning project. There are a few constraints on the nature of the allowed project. \n",
    "- The problem addressed will not be a \"toy problem\" or \"common training students problem\" like 8-Queens or a small Traveling Salesman Problem or similar\n",
    "- If its the kind of problem (e.g., RL) that interacts with a simulator or live task, then the problem will have a reasonably complex action space. For instance, a wupus world kind of thing with a 9x9 grid is definitely too small.  A simulated mountain car with a less complex 2-d road and simplified dynamics seems like a fairly low achievement level.  A more complex 3-d mountain car simulation with large extent and realistic dynamics, sure sounds great!\n",
    "- If its the kind of problem that uses a dataset, then the dataset will have >1k observations and >5 variables. I'd prefer more like >10k observations and >10 variables. A general rule is that if you have >100x more observations than variables, your solution will likely generalize a lot better. The goal of training an unsupervised machine learning model is to learn the underlying pattern in a dataset in order to generalize well to unseen data, so choosing a large dataset is very important.\n",
    "- The project must include some elements we talked about in the course\n",
    "- The project will include a model selection and/or feature selection component where you will be looking for the best setup to maximize the performance of your AI system. Generally RL tasks may require a huge amount of training, so extensive grid search is unlikely to be possible. However expoloring a few reasonable hyper-parameters may still be possible. \n",
    "- You will evaluate the performance of your AI system using more than one appropriate metric\n",
    "- You will be writing a report describing and discussing these accomplishments\n",
    "\n",
    "\n",
    "Feel free to delete this description section when you hand in your proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "Hopefully your team is at least this good. Obviously you should replace these with your names.\n",
    "\n",
    "- Ashesh Kaji\n",
    "- Abhay Anand\n",
    "- Varun Pillai\n",
    "- Ansh Bhatnagar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "The goal of this project is to train a Reinforcement Learning (RL) Classifier on autonomous vehicles. We plan to use DonkeyCar Simulator to navigate the \"Warren Feild\" track and collect our data. The data we plan to collect includes visual input from the front-facing camera, speed and steering angle change. These will be collected during training in the simulated environment. We implement and compare two deep RL algorithms: Actor-Critic and Proximal Policy Optimization (PPO), both designed for continuous action spaces. Moreover we plan to include a simple Q learning model to show how inefficient/ ineffective it is for more complex problems. We will use the gathered data to train agents to take optimal actions such as steering left/right, acceleration, and braking based on the cars current position. Performance will be evaluated using key metrics such as cumulative reward, lap completion time, and DonkeySim's own accuracy rating. By comparing these metrics across different models and training scenarios, we aim to determine which RL method provides the most robust and efficient control for autonomous driving in simulated environments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Background\n",
    "\n",
    "Fill in the background and discuss the kind of prior work that has gone on in this research area here. **Use inline citation** to specify which references support which statements.  You can do that through HTML footnotes (demonstrated here). I used to reccommend Markdown footnotes (google is your friend) because they are simpler but recently I have had some problems with them working for me whereas HTML ones always work so far. So use the method that works for you, but do use inline citations.\n",
    "\n",
    "Here is an example of inline citation. After government genocide in the 20th century, real birds were replaced with surveillance drones designed to look just like birds<a name=\"lorenz\"></a>[<sup>[1]</sup>](#lorenznote). Use a minimum of 3 to 5 citations, but we prefer more <a name=\"admonish\"></a>[<sup>[2]</sup>](#admonishnote). You need enough citations to fully explain and back up important facts. \n",
    "\n",
    "Remeber you are trying to explain why someone would want to answer your question or why your hypothesis is in the form that you've stated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Autonomous driving has emerged as a rapidly evolving field, as recent advances in computing power\n",
    "and machine learning continue to push the boundaries of vehicle autonomy<a name=\"koutnik\"></a>[<sup>[1]</sup>](#koutniknote).\n",
    "A major breakthrough was the introduction of deep reinforcement learning methods capable of learning\n",
    "complex control policies from high-dimensional data, such as pixel inputs<a name=\"mnih\"></a>[<sup>[2]</sup>](#mnihnote).\n",
    "Moreover, open-source platforms like CARLA provide realistic urban scenarios, enabling researchers\n",
    "to safely collect training data in highly variable traffic and weather conditions<a name=\"carla\"></a>[<sup>[3]</sup>](#carlanote).\n",
    "Concurrent work in policy optimization techniques, such as Proximal Policy Optimization (PPO),\n",
    "has improved training stability for continuous control tasks, making them more suitable for real-world\n",
    "driving problems<a name=\"schulman\"></a>[<sup>[4]</sup>](#schulmannotenote).\n",
    "\n",
    "Why is this important? Autonomous driving stands to improve road safety, increase mobility,\n",
    "and reduce congestion. However, it also introduces unique challenges in perception, planning,\n",
    "and control. Studying reinforcement learning in this domain is crucial for advancing algorithms\n",
    "that can handle high-dimensional state spaces and continuous action controls, ultimately bringing\n",
    "us closer to reliable self-driving cars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Clearly describe the problem that you are solving. Avoid ambiguous words. The problem described should be well defined and should have at least one ML-relevant potential solution. Additionally, describe the problem thoroughly such that it is clear that the problem is quantifiable (the problem can be expressed in mathematical or logical terms), measurable (the problem can be measured by some metric and clearly observed), and replicable (the problem can be reproduced and occurs more than once)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "## Original Instructions\n",
    "You should have a strong idea of what dataset(s) will be used to accomplish this project. \n",
    "\n",
    "If you know what (some) of the data you will use, please give the following information for each dataset:\n",
    "- link/reference to obtain it\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "- what an observation consists of\n",
    "- what some critical variables are, how they are represented\n",
    "- any special handling, transformations, cleaning, etc will be needed\n",
    "\n",
    "If you don't yet know what your dataset(s) will be, you should describe what you desire in terms of the above bullets.\n",
    "\n",
    "## Data\n",
    "We will be using the CARLA open-source gym environment for our project. The CARLA simulator is a platform for autonomous driving research, providing a realistic urban driving environment with various weather conditions, traffic scenarios, and sensor modalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "## original instructions\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Why might your solution work? Make sure to describe how the solution will be tested.  \n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. \n",
    "\n",
    "## Proposed Solution\n",
    "We will implement a deep reinforcement learning (RL) approach to train an agent to navigate the CARLA environment. The RL agent will learn to make decisions based on the state of the environment, which includes visual input from the front-facing camera, speed, and steering angle. We will implement three algprithms, one monte carlo based, and two deep learning based:\n",
    "1. **Monte Carlo Q-Learning**: A simple Q-learning algorithm that uses a table to store the Q-values for each state-action pair. This approach will serve as a baseline to compare the performance of more complex algorithms.\n",
    "2. **Actor-Critic**: A deep RL algorithm that uses two neural networks: an actor network to select actions and a critic network to evaluate the actions taken. The actor network will be trained to maximize the expected cumulative reward, while the critic network will be trained to minimize the difference between the predicted and actual rewards.\n",
    "3. **Proximal Policy Optimization (PPO)**: A more advanced deep RL algorithm that uses a surrogate objective function to optimize the policy. PPO is known for its stability and sample efficiency, making it suitable for continuous action spaces like autonomous driving.\n",
    "\n",
    "\n",
    "## Porposed Code (has not been run, unsure if it will work)\n",
    "\n",
    "```python\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from environment import CarlaEnv\n",
    "import random\n",
    "\n",
    "def preprocess(state):\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "    return state_tensor\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(7 * 7 * 64, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Linear(512, num_actions)\n",
    "        self.critic = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)\n",
    "        features = self.fc(features)\n",
    "        logits = self.actor(features)\n",
    "        value = self.critic(features)\n",
    "        return logits, value\n",
    "\n",
    "class PPOAgent(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: CarlaEnv,\n",
    "        lr: float = 3e-4,\n",
    "        gamma: float = 0.99,\n",
    "        lam: float = 0.95,\n",
    "        clip_epsilon: float = 0.2,\n",
    "        rollout_steps: int = 2048,\n",
    "    ):\n",
    "        super(PPOAgent, self).__init__()\n",
    "        self.env = env\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.rollout_steps = rollout_steps\n",
    "        self.num_actions = self.env.action_space.n\n",
    "\n",
    "        self.policy_net = PolicyNetwork(self.num_actions)\n",
    "        self.save_hyperparameters(ignore=[\"env\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.policy_net(x)\n",
    "\n",
    "    def collect_rollout(self):\n",
    "        states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []\n",
    "        state = self.env.reset()\n",
    "        for _ in range(self.rollout_steps):\n",
    "            state_tensor = preprocess(state).to(self.device)\n",
    "            logits, value = self.policy_net(state_tensor.unsqueeze(0))\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            action = torch.multinomial(probs, num_samples=1)\n",
    "            log_prob = torch.log(probs.gather(1, action) + 1e-8)\n",
    "            \n",
    "            next_state, reward, done, _ = self.env.step(action.item())\n",
    "\n",
    "            states.append(state_tensor.unsqueeze(0))\n",
    "            actions.append(action)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=self.device))\n",
    "            dones.append(torch.tensor([float(done)], dtype=torch.float, device=self.device))\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state = self.env.reset()\n",
    "\n",
    "        rollout = {\n",
    "            'states': torch.cat(states, dim=0),\n",
    "            'actions': torch.cat(actions, dim=0),\n",
    "            'rewards': torch.cat(rewards, dim=0),\n",
    "            'dones': torch.cat(dones, dim=0),\n",
    "            'log_probs': torch.cat(log_probs, dim=0),\n",
    "            'values': torch.cat(values, dim=0)\n",
    "        }\n",
    "        return rollout\n",
    "\n",
    "    def compute_gae(self, rewards, values, dones):\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        values = values.squeeze(-1)\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            next_value = values[step + 1] if step < len(rewards) - 1 else 0\n",
    "            delta = rewards[step] + self.gamma * (1 - dones[step]) * next_value - values[step]\n",
    "            gae = delta + self.gamma * self.lam * (1 - dones[step]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "        advantages = torch.stack(advantages)\n",
    "        returns = advantages + values\n",
    "        return advantages, returns\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        rollout = self.collect_rollout()\n",
    "        advantages, returns = self.compute_gae(\n",
    "            rollout['rewards'], rollout['values'], rollout['dones']\n",
    "        )\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        logits, values_new = self.policy_net(rollout['states'])\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        new_log_probs = torch.log(probs.gather(1, rollout['actions']) + 1e-8)\n",
    "        ratio = torch.exp(new_log_probs - rollout['log_probs'])\n",
    "\n",
    "        advantages = advantages.unsqueeze(1)\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1.0 - self.clip_epsilon, 1.0 + self.clip_epsilon) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        returns = returns.detach()\n",
    "        value_loss = F.mse_loss(values_new.squeeze(), returns)\n",
    "        loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "        self.log(\"policy_loss\", policy_loss, prog_bar=True)\n",
    "        self.log(\"value_loss\", value_loss, prog_bar=True)\n",
    "        self.log(\"total_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.policy_net.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = CarlaEnv()\n",
    "    model = PPOAgent(env)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=100,\n",
    "        gpus=1 if torch.cuda.is_available() else 0,\n",
    "        log_every_n_steps=10,\n",
    "        checkpoint_callback=True,\n",
    "    )\n",
    "    trainer.fit(model, train_dataloader=[None])\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# DQN implementation approach\n",
    "def preprocess(state):\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "    return state_tensor\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(7 * 7 * 64, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.out = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)\n",
    "        features = self.fc(features)\n",
    "        q_values = self.out(features)\n",
    "        return q_values\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.cat(states),\n",
    "            torch.tensor(actions, dtype=torch.long).unsqueeze(1),\n",
    "            torch.tensor(rewards, dtype=torch.float32).unsqueeze(1),\n",
    "            torch.cat(next_states),\n",
    "            torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQNAgent(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: CarlaEnv,\n",
    "        lr: float = 1e-3,\n",
    "        gamma: float = 0.99,\n",
    "        buffer_capacity: int = 10000,\n",
    "        batch_size: int = 32,\n",
    "        epsilon_start: float = 1.0,\n",
    "        epsilon_end: float = 0.1,\n",
    "        epsilon_decay: int = 10000,\n",
    "        target_update_freq: int = 1000\n",
    "    ):\n",
    "        super(DQNAgent, self).__init__()\n",
    "        self.env = env\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.target_update_freq = target_update_freq\n",
    "\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.online_net = QNetwork(self.num_actions)\n",
    "        self.target_net = QNetwork(self.num_actions)\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.total_steps = 0\n",
    "\n",
    "        self.save_hyperparameters(ignore=[\"env\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.online_net(x)\n",
    "\n",
    "    def epsilon_by_step(self, step):\n",
    "        return self.epsilon_end + (self.epsilon_start - self.epsilon_end) * np.exp(-step / self.epsilon_decay)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        if not hasattr(self.env, \"current_state\"):\n",
    "            state = self.env.reset()\n",
    "            self.env.current_state = state\n",
    "        else:\n",
    "            state = self.env.current_state\n",
    "\n",
    "        state_tensor = preprocess(state).to(self.device).unsqueeze(0)\n",
    "        epsilon = self.epsilon_by_step(self.total_steps)\n",
    "        \n",
    "        if random.random() < epsilon:\n",
    "            action = random.randrange(self.num_actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.online_net(state_tensor)\n",
    "                action = q_values.argmax(dim=1).item()\n",
    "        \n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "        next_state_tensor = preprocess(next_state).to(self.device).unsqueeze(0)\n",
    "        self.replay_buffer.push(state_tensor, action, reward, next_state_tensor, done)\n",
    "\n",
    "        if done:\n",
    "            state = self.env.reset()\n",
    "        else:\n",
    "            state = next_state\n",
    "        self.env.current_state = state\n",
    "        self.total_steps += 1\n",
    "\n",
    "        if self.total_steps % self.target_update_freq == 0:\n",
    "            self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "\n",
    "        current_q = self.online_net(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            max_next_q = self.target_net(next_states).max(dim=1, keepdim=True)[0]\n",
    "            target_q = rewards + self.gamma * max_next_q * (1 - dones)\n",
    "        \n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        self.log(\"loss\", loss, prog_bar=True)\n",
    "        self.log(\"epsilon\", epsilon, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.online_net.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = CarlaEnv()\n",
    "    model = DQNAgent(env)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=100,\n",
    "        gpus=1 if torch.cuda.is_available() else 0,\n",
    "        log_every_n_steps=10,\n",
    "        checkpoint_callback=True,\n",
    "    )\n",
    "    trainer.fit(model, train_dataloader=[None])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Evaluation Metrics\n",
    "\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "For this study, we will focus on several key metrics that quantify performance and robustness\n",
    "of autonomous driving RL models. The following metrics are commonly referenced in prior work\n",
    "and can be applied to both simpler baselines (e.g., Monte Carlo Q-learning) and more advanced\n",
    "methods (Actor-Critic, PPO, etc.):\n",
    "\n",
    "1. Cumulative Reward\n",
    "   ------------------\n",
    "   Definition:\n",
    "     The total sum of rewards obtained over an episode or across training. A higher cumulative\n",
    "     reward typically indicates more efficient navigation and adherence to safety constraints\n",
    "     (e.g., staying on track, avoiding collisions)<a name=\"mnih\"></a>[<sup>[2]</sup>](#mnihnote).\n",
    "\n",
    "   Mathematical Representation:\n",
    "     R_total = Σ (r_t)  for  t = 0 to T,\n",
    "     where r_t is the reward at time step t and T is the total number of steps in the episode.\n",
    "\n",
    "2. Lap Completion Time\n",
    "   -------------------\n",
    "   Definition:\n",
    "     The time taken by the agent to complete a single lap (or multiple laps) in simulation.\n",
    "     Lower times indicate faster and often more optimal driving behavior. Platforms like\n",
    "     CARLA frequently log this metric for performance evaluation<a name=\"carla\"></a>[<sup>[3]</sup>](#carlanote).\n",
    "\n",
    "   Mathematical/Logical Form:\n",
    "     - Time is measured in seconds from the lap start to completion.\n",
    "     - This metric is a real-valued measure of efficiency (↓ is better).\n",
    "\n",
    "3. Collision Count\n",
    "   ---------------\n",
    "   Definition:\n",
    "     The number of collisions or off-track events per episode. This directly relates to safety\n",
    "     and the agent’s ability to avoid obstacles. Dosovitskiy et al. highlight collision metrics\n",
    "     as a core measure in CARLA simulations<a name=\"carlaAgain\"></a>[<sup>[3]</sup>](#carlanote).\n",
    "\n",
    "   Quantifiable Measure:\n",
    "     collision_count = Σ (binary_collision_indicator_t)\n",
    "       for t = 0 to T\n",
    "       (1 if a collision occurs at time t, otherwise 0)\n",
    "\n",
    "4. Tracking Error\n",
    "   --------------\n",
    "   Definition:\n",
    "     Measures how closely the agent adheres to the desired lane or trajectory. This metric\n",
    "     is particularly useful in environments like CARLA or DonkeyCar, which can log lateral\n",
    "     deviation from the lane center<a name=\"koutnik\"></a>[<sup>[1]</sup>](#koutniknote).\n",
    "\n",
    "   Mathematical Representation (example for lateral deviation):\n",
    "     tracking_error = (1 / T) * Σ (|lane_center - agent_position_t|)\n",
    "       for t = 1 to T\n",
    "\n",
    "5. Policy Convergence Rate\n",
    "   ------------------------\n",
    "   Definition:\n",
    "     Reflects how quickly each model converges to a stable policy. Methods like Proximal Policy\n",
    "     Optimization (PPO) often track the KL divergence between successive policy updates to gauge\n",
    "     stability<a name=\"schulman\"></a>[<sup>[4]</sup>](#schulmannotenote).\n",
    "\n",
    "   Potential Approach:\n",
    "     - Observe episode rewards over training; define convergence when ∆Reward < ε for N consecutive episodes.\n",
    "     - Alternatively, use KL divergence thresholds between policy iterations.\n",
    "\n",
    "By comparing these quantifiable metrics across algorithms and experimental conditions,\n",
    "we can gauge which approaches best balance safety, efficiency, and reliable control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination. Get creative!\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put things here that cement how you will interact/communicate as a team, how you will handle conflict and difficulty, how you will handle making decisions and setting goals/schedule, how much work you expect from each other, how you will handle deadlines, etc...\n",
    "* Regular updates via WhatsApp, weekly check-ins, quick status updates when problems/issues arise \n",
    "* Set realistic deadlines, and communicate on time if a problem comes up\n",
    "* Decision-making should be done collaboratively, discussions for major decisions\n",
    "* Address conflicts respectfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this with something meaningful that is appropriate for your needs. It doesn't have to be something that fits this format.  It doesn't have to be set in stone... \"no battle plan survives contact with the enemy\". But you need a battle plan nonetheless, and you need to keep it updated so you understand what you are trying to accomplish, who's responsible for what, and what the expected due dates are for each item.\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 1/20  |  1 PM |  Brainstorm topics/questions (all)  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 1/26  |  10 AM |  Do background research on topic (Pelé) | Discuss ideal dataset(s) and ethics; draft project proposal | \n",
    "| 2/1  | 10 AM  | Edit, finalize, and submit proposal; Search for datasets (Beckenbaur)  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 2/14  | 6 PM  | Import & Wrangle Data ,do some EDA (Maradonna) | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 2/23  | 12 PM  | Finalize wrangling/EDA; Begin programming for project (Cruyff) | Discuss/edit project code; Complete project |\n",
    "| 3/13  | 12 PM  | Complete analysis; Draft results/conclusion/discussion (Carlos)| Discuss/edit full project |\n",
    "| 3/19  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"koutniknote\"></a>1.[^](#koutnik): Koutník, J., Schmidhuber, J., & Gómez, F. (2014).\n",
    "Evolving deep unsupervised convolutional networks for vision-based reinforcement learning.\n",
    "[arXiv preprint](https://arxiv.org/abs/1312.6120)<br>\n",
    "\n",
    "<a name=\"mnihnote\"></a>2.[^](#mnih): Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., et al. (2015).\n",
    "Human-level control through deep reinforcement learning. *Nature*, 518(7540), 529–533.\n",
    "[Link](https://www.nature.com/articles/nature14236)<br>\n",
    "\n",
    "<a name=\"carlanote\"></a>3.[^](#carla): Dosovitskiy, A., Ros, G., Codevilla, F., López, A., & Koltun, V. (2017).\n",
    "CARLA: An open urban driving simulator.\n",
    "[arXiv preprint](https://arxiv.org/abs/1711.03938)<br>\n",
    "\n",
    "<a name=\"schulmannotenote\"></a>4.[^](#schulman): Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017).\n",
    "Proximal Policy Optimization Algorithms.\n",
    "[arXiv preprint](https://arxiv.org/abs/1707.06347)<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
