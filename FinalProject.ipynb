{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **AutoBots: Using Reinforcement Learning with Carla & DonkeySim**\n",
    "\n",
    "### Group members\n",
    "\n",
    "- Abhay Anand\n",
    "- Ashesh Kaji\n",
    "- Varun Pillai\n",
    "- Ansh Bhatnagar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "The goal of this project is to train a Reinforcement Learning (RL) Classifier on autonomous vehicles. We plan to use DonkeyCar Simulator to navigate the \"Warren Feild\" track and collect our data. The data we plan to collect includes visual input from the front-facing camera, speed and steering angle change. These will be collected during training in the simulated environment. We implement and compare two deep RL algorithms: Actor-Critic and Proximal Policy Optimization (PPO), both designed for continuous action spaces. Moreover we plan to include a simple Q learning model to show how inefficient/ ineffective it is for more complex problems. We will use the gathered data to train agents to take optimal actions such as steering left/right, acceleration, and braking based on the cars current position. Performance will be evaluated using key metrics such as cumulative reward, lap completion time, and DonkeySim's own accuracy rating. By comparing these metrics across different models and training scenarios, we aim to determine which RL method provides the most robust and efficient control for autonomous driving in simulated environments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Autonomous driving has rapidly advanced due to improvements in computing power and machine learning, particularly reinforcement learning (RL), which enables autonomous agents to learn optimal control strategies through trial and error. Unlike traditional rule-based or supervised learning methods, RL-based approaches can dynamically adapt to new environments and uncertain conditions, making them well-suited for self-driving applications <a name=\"kiran\"></a>[<sup>[1]</sup>](#kirannote). However, a significant challenge in RL-driven autonomous navigation is the simulation-to-reality gap, where models trained in virtual environments struggle to perform reliably in real-world settings due to differences in sensor noise, road textures, and unexpected obstacles <a name=\"peng\"></a>[<sup>[2]</sup>](#pengnote). Research on domain adaptation and transfer learning continues to address this issue by fine-tuning RL policies with real-world data <a name=\"taylor\"></a>[<sup>[3]</sup>](#taylornote).  \n",
    "\n",
    "Simulation environments play a crucial role in training RL-based autonomous driving models. DonkeyCar Simulator (DonkeySim) provides a lightweight platform for testing self-driving models on controlled tracks and is widely used due to its accessibility and ease of experimentation <a name=\"donkeysim\"></a>[<sup>[4]</sup>](#donkeysimnote). The simulator provides key sensory inputs such as camera feeds, speed readings, and steering angles, enabling the development of reinforcement learning pipelines without requiring physical hardware. For more complex urban driving scenarios, CARLA offers a high-fidelity simulation environment with dynamic traffic, weather conditions, and diverse road layouts <a name=\"carla\"></a>[<sup>[5]</sup>](#carlanote). CARLA allows for more extensive testing of RL models in realistic settings, making it a crucial tool for benchmarking autonomous navigation systems.  \n",
    "\n",
    "Recent advancements in policy optimization techniques have improved training stability and efficiency in reinforcement learning for autonomous driving. Proximal Policy Optimization (PPO) has emerged as a preferred method for continuous control tasks due to its ability to balance exploration and exploitation while preventing overly large policy updates <a name=\"ppo\"></a>[<sup>[6]</sup>](#pponote). Additionally, Actor-Critic methods provide an effective framework for reinforcement learning by combining value-based and policy-based learning, resulting in more stable and informed decision-making in autonomous navigation tasks <a name=\"actorcritic\"></a>[<sup>[7]</sup>](#actorcriticnote).  \n",
    "\n",
    "Self-driving technology has the potential to improve road safety, efficiency, and accessibility, but achieving reliable autonomy remains a challenge. Reinforcement learning-based methods, combined with improvements in simulation environments, optimized reward functions, and safer real-world deployment strategies, can contribute to the refinement of autonomous navigation systems. Continued research on policy optimization, explainable AI, and real-world generalization will be essential for making self-driving technology both practical and ethically responsible.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Autonomous navigation for industrial and factory environments requires precise and efficient vehicle control to ensure safe and timely transportation of goods. Traditional rule-based and vision-based approaches struggle with real-time adaptability and robustness in dynamic settings where numerous unexpected obstacles may arrise due to minor mishaps. Through our project, we aim to develop a deep reinforcement learning (RL) model that enables autonomous vehicles to navigate factory environments using only LiDAR data as input. By leveraging reinforcement learning techniques, particularly Proximal Policy Optimization (PPO) and Actor-Critic methods, we aim to train a model capable of handling continuous action spaces while minimizing computational complexity. Our goal is to create an efficient, collision-free, and fast driving policy that enhances safety, accuracy, and cost-effectiveness in automated logistics and manufacturing operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "For this reinforcement learning project, we generated training data through interactions with the CARLA simulator, a high-fidelity environment designed for autonomous vehicle research. Unlike traditional datasets, our approach relies on real-time sensory inputs from the simulator, with LiDAR data serving as the cornerstone of our state representation.\n",
    "\n",
    "### LiDAR Data Collection\n",
    "- **LiDAR Sensor**: We attached a LiDAR sensor to the vehicle in CARLA, configured with a 50-meter range to capture point cloud data representing distances to surrounding objects.\n",
    "- **Data Processing**: The raw LiDAR point cloud is processed to create a compact yet informative state:\n",
    "  - The point cloud is divided into 8 sectors, each spanning 45 degrees around the vehicle (from -180° to 180°).\n",
    "  - For each sector, we compute the minimum and maximum distances to obstacles, yielding 16 features (8 minimums and 8 maximums). If no points are detected in a sector, a default distance of 50 meters is used.\n",
    "  - This approach reduces computational complexity while preserving spatial awareness, enabling the agent to detect obstacles in all directions.\n",
    "- **State Augmentation**: The LiDAR-derived features are augmented with two additional variables:\n",
    "  - Distance to the next waypoint, calculated as the Euclidean distance from the vehicle’s current position to the target waypoint’s location.\n",
    "  - Angular difference between the vehicle’s current heading (yaw) and the direction to the next waypoint, normalized to [-180°, 180°].\n",
    "  - This results in an 18-dimensional state vector (16 LiDAR features + 2 navigation features).\n",
    "- **Implementation Details**: In our `CarlaEnvWrapper` class, the `_get_state` method processes LiDAR data by converting raw points into polar coordinates (angles and distances), binning them into sectors, and computing min/max distances. The state is updated synchronously with each simulation tick to ensure consistency.\n",
    "\n",
    "By using LiDAR data instead of camera inputs, we avoid the computational overhead of convolutional neural networks, making our model efficient for real-time industrial applications while maintaining robust environmental perception.\n",
    "\n",
    "## Old (needs to be updated for donkey sim and then removed)\n",
    "For this reinforcement learning project, we will not use a pre-existing dataset to train our agent but we will instead rely on generating training data using the DonkeySim environment, which is an application with pre-made racing tracks for testing autonomous vehicles. The simulator software itself provides essential sensory inputs, including a front-facing camera stream, speed readings, obstacle-hit counters, and steering angle data, which will serve as the basis for our state space for our models (We may tinker with this and not use all of the listed data sources, or we may even add more. Since this is our first time using DonkeySim, we will have to experiment as we go a little bit). \n",
    "\n",
    "The agent will interact with the environment by taking actions such as steering, accelerating, and braking, and it will receive rewards based on reward function's pre-defined criteria, such as staying on track and minimizing lap time. During training, we currently plan to collect and store experience tuples (state, action, reward, next-state) to teach the agent to optimize the learning process. To enhance performance, we may experiment with different reward functions and data augmentation techniques, such as varying tracks, messing around with the weights for the reward function, and penaliing sharp turns to improve model generalization. By training our agent iteratively within DonkeySim, we ensure that our approach remains scalable and adaptable without the need for an external dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "### 1. State Representation\n",
    "- The vehicle relies solely on LiDAR data, processed into an 18-dimensional state vector (16 LiDAR features + 2 waypoint features), as detailed in the Data section.\n",
    "- This design ensures a low computational footprint and real-time decision-making capability.\n",
    "\n",
    "### 2. Action Space\n",
    "- The action space is continuous, with two dimensions: throttle/brake ([-1, 1], where positive values are throttle and negative are brake) and steering ([-1, 1]).\n",
    "- This allows the agent to dynamically adjust speed and direction, learning the interplay between velocity and turning for smooth navigation.\n",
    "\n",
    "### 3. Reinforcement Learning Approach\n",
    "- We implemented Proximal Policy Optimization (PPO) using PyTorch Lightning, training the agent in the CARLA simulator.\n",
    "- PPO balances exploration and exploitation, making it ideal for our continuous control task.\n",
    "#### Neural Network Architecture\n",
    "The PPO agent relies on two distinct neural networks: the actor, which determines the policy (action selection), and the critic, which estimates the value function. These networks are designed as multi-layer perceptrons (MLPs) with the following structures:\n",
    "\n",
    "- **Actor Network**:\n",
    "  - **Structure**: A four-layer MLP:\n",
    "    - **Input Layer**: 18 neurons, corresponding to the state dimension (e.g., sensor data, velocity, etc.).\n",
    "    - **Hidden Layers**: Three layers with 256, 256, and 128 neurons, respectively, each followed by Tanh activation functions.\n",
    "    - **Output Layer**: 4 neurons, representing the mean and log standard deviation (log_std) for two continuous actions: throttle and steering (2 neurons per action).\n",
    "  - **Output Processing**:\n",
    "    - **Throttle Mean**: Passed through a sigmoid function to produce values in the range [0,1], biasing the agent toward forward movement.\n",
    "    - **Steering Mean**: Passed through a tanh function to produce values in the range [-1,1], enabling smooth left and right turns.\n",
    "    - **Standard Deviation**: The log_std outputs are exponentiated, clamped, and constrained to a minimum value to ensure sufficient exploration during training.\n",
    "  - **Initialization**: Weights are initialized using Xavier uniform initialization, and biases are set to zero.\n",
    "  - **Purpose**: The Tanh activations help stabilize policy updates, while the split output design accommodates the continuous action space required for driving control.\n",
    "\n",
    "- **Critic Network**:\n",
    "  - **Structure**: A four-layer MLP:\n",
    "    - **Input Layer**: 18 neurons, matching the state dimension.\n",
    "    - **Hidden Layers**: Three layers with 256, 256, and 128 neurons, respectively, each followed by ReLU activation functions.\n",
    "    - **Output Layer**: 1 neuron, providing the value estimate for the given state.\n",
    "  - **Initialization**: Weights are initialized using Xavier uniform initialization, and biases are set to zero.\n",
    "  - **Purpose**: The ReLU activations support effective value approximation, enabling the critic to provide stable and accurate estimates of the state’s expected return.\n",
    "\n",
    "### 4. Reward Function Design\n",
    "The reward function evolved iteratively to guide the agent toward safe, efficient, and route-following behavior:\n",
    "\n",
    "- **Initial Reward Function**:\n",
    "  - **Collision Avoidance**: A penalty of -50 was applied for collisions to prioritize safety.\n",
    "  - **Speed Maintenance**: Reward was proportional to distance traveled per step, encouraging forward movement.\n",
    "  - This basic design promoted movement while avoiding obstacles but lacked route guidance.\n",
    "\n",
    "- **Intermediate Reward Function**:\n",
    "  - **Lane Discipline**: Added a -1 penalty for lane invasions to keep the vehicle within track boundaries.\n",
    "  - **Speed Regulation**: Introduced a target speed of 30 km/h, with a penalty (-0.1 * |speed - target|) for deviations, and an additional -1 penalty for speeds below 5 km/h to prevent stalling.\n",
    "  - **Steering Smoothness**: Penalized large steering actions (-0.5 * |steering|) when speed was below 5 km/h to reduce erratic behavior at low speeds.\n",
    "  - This improved track adherence and consistency but didn’t ensure progress along a specific path.\n",
    "\n",
    "- **Final Reward Function**:\n",
    "  - **Waypoint Following**: Added a reward based on proximity to the next waypoint (max(0, 5 - distance/10)), encouraging route adherence.\n",
    "  - **Heading Alignment**: Included a bonus (max(0, 1 - |angle_diff|/180)) for aligning the vehicle’s heading with the waypoint direction, promoting smoother turns.\n",
    "  - **Progress Reward**: Retained distance traveled as a base reward, augmented by waypoint incentives.\n",
    "  - **Safety Penalties**: Kept collision (-50) and lane invasion (-1) penalties.\n",
    "  - **Stuck Detection**: Penalized (-2) if the vehicle’s position varied by less than 1 meter over 20 steps, preventing circular or stagnant behavior.\n",
    "  - Implemented in `CarlaEnvWrapper.step`, this final version balances safety, efficiency, and navigation.\n",
    "\n",
    "### 5. Deployment and Applications\n",
    "#### Setup deployment details\n",
    "A full list complete setup details can be found in the code repo's README. Essentially the following items must be setup:\n",
    "1. Carla 0.9.15 must be setup on a gpu based machine, as well as the python api for the same version for full compatibility\n",
    "2. Python must be setup with respective packages\n",
    "3. The car is setup in the default map with the default settings. It is a real world simulation, but with no NPCs to reduce complexity given the projects scale.\n",
    "#### Potential Future applications\n",
    "- The trained PPO model can optimize logistics in factory settings, enabling autonomous vehicles to transport goods safely and efficiently along predefined routes.\n",
    "- This approach reduces costs and enhances precision in industrial automation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "#### 1. Cumulative Reward\n",
    "- **Definition**: The total reward accumulated over an episode, calculated based on the reward function defined in your project.\n",
    "- **Significance**: This metric reflects the overall performance of the agent. Higher cumulative rewards indicate better navigation, fewer collisions, and more effective adherence to the intended route. It serves as a primary indicator of policy improvement during training.\n",
    "\n",
    "#### 2. Collision Rate\n",
    "- **Definition**: The frequency of collisions with obstacles or boundaries during an episode.\n",
    "- **Significance**: A lower collision rate is desirable, as it demonstrates the agent’s ability to navigate safely and avoid hazards. This metric is critical for evaluating the safety performance of the driving policy.\n",
    "\n",
    "#### 3. Distance Traveled\n",
    "- **Definition**: The total distance covered by the vehicle over the course of an episode.\n",
    "- **Significance**: When paired with a low collision rate, a higher distance traveled suggests efficient and effective navigation. This metric highlights the agent’s progress and ability to follow the desired path.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Reference the videos here: https://drive.google.com/drive/folders/1K7cDpq456Woh-fQq7A6EAiD3Nz-YboTQ?usp=sharing\n",
    "\n",
    "\n",
    "The primary objective of this study is to demonstrate that a well-designed reward function is essential for effective reinforcement learning (RL)-based autonomous navigation. Additional considerations include the utility of LiDAR data for state representation and the appropriateness of PPO for continuous control tasks. We evaluate the PPO agent's performance using two distinct reward models: a simpler reward-based model and an older, more complicated reward-based model. The analysis centers on key performance metrics, including episode rewards, distance traveled, episode lengths, and reward components, derived from the training progress at epoch 50 for the older model and the rewards vs. steps relationship for the simpler model.\n",
    "\n",
    "---\n",
    "#### Subsection 1: Performance of the Simpler Reward-Based Model\n",
    "<img src=\"Carla_PPO_agent/training_progress_epoch_50.png\" alt=\"Training Progress Epoch 50\" />\n",
    "The performance of the PPO agent with the simpler reward-based model is illustrated in a scatter plot titled \"Episode Reward vs Steps (Colored by Epoch),\" which tracks episode rewards against the number of steps across epochs 35 to 72.\n",
    "\n",
    "- **Early Epochs (35-40)**:\n",
    "  - During the initial training phase (epochs 35-40), episode rewards are consistently low, ranging between -6000 and -4000. At around 100 steps, rewards frequently reach -6000, reflecting poor performance early in training. As steps increase to 200, rewards remain predominantly negative, with significant variability—some points improve to -1000, while others drop back to -5000 or lower.\n",
    "\n",
    "- **Mid Epochs (41-55)**:\n",
    "  - In the mid-training phase (epochs 41-55), the agent's performance begins to improve. Between 200 and 300 steps, episode rewards cluster between -4000 and -2000, with fewer points falling below -5000. Variability persists, with some episodes achieving rewards as high as -1000, while others remain around -4000, indicating inconsistent progress.\n",
    "\n",
    "- **Later Epochs (56-72)**:\n",
    "  - By the later epochs (56-72), further improvement is evident. Between 300 and 450 steps, rewards concentrate between -3000 and -1000, with several points nearing 0 (e.g., around -500 at 400-450 steps). However, some outliers still drop to -4000, suggesting that the agent has not yet achieved fully stable performance.\n",
    "\n",
    "**Key Observation**:\n",
    "The scatter plot reveals a clear upward trend in episode rewards as both steps and epochs increase, demonstrating that the PPO agent is learning and refining its navigation abilities under the simpler reward model. Nevertheless, the persistent variability in rewards indicates that additional training or model refinement may be required to achieve consistent and optimal performance.\n",
    "\n",
    "---\n",
    "\n",
    "#### Subsection 2: Performance of the Older, More Complicated Reward-Based Model (Training Progress at Epoch 50)\n",
    "<img src=\"Carla_PPO_agent/reward_vs_steps_by_epoch.png\" alt=\"Training Progress Epoch 50\" />\n",
    "The older, more complicated reward-based model’s performance is assessed using four charts depicting training progress over 100 epochs. Here, we focus specifically on the agent's behavior at epoch 50.\n",
    "\n",
    "- **Episode Rewards**:\n",
    "  - At epoch 50, the total reward is approximately 2,000. This follows a peak of around 10,000 at episode 40 and a subsequent decline. Across the 100 epochs, rewards fluctuate widely, with notable spikes at episode 10 (around 20,000), episode 60 (around 15,000), and episode 80 (around 5,000), but they generally hover around 2,000 outside these peaks.\n",
    "\n",
    "- **Distance Traveled**:\n",
    "  - The distance traveled at epoch 50 reaches approximately 120 meters, a significant peak compared to surrounding episodes, which typically range between 20 and 40 meters. Other prominent peaks include episode 10 (120 meters), episode 30 (100 meters), episode 70 (80 meters), and episode 90 (140 meters).\n",
    "\n",
    "- **Episode Lengths**:\n",
    "  - At epoch 50, the episode length is around 300 steps, relatively modest compared to earlier peaks like episode 10 (900 steps). Episode lengths generally fluctuate between 200 and 400 steps, with occasional spikes, such as 600 steps at episode 60 and 700 steps at episode 80.\n",
    "\n",
    "- **Reward Components**:\n",
    "  - The total reward at epoch 50 is dominated by the \"progress\" component (approximately 2,000), with a minor contribution from the \"longevity\" component (around 500). Other components—collision, lane invasion, lane centering, heading, speed, steering smoothness, survival bonus, and completion bonus—remain near zero, indicating negligible influence on the total reward.\n",
    "\n",
    "**Key Observation**:\n",
    "At epoch 50, the older model yields a total reward of 2,000, with a notable distance traveled of 120 meters. The overwhelming contribution of the \"progress\" reward component suggests that the agent is primarily incentivized to move forward, while safety and efficiency metrics (e.g., collision avoidance, lane discipline) play a minimal role. This imbalance may result in suboptimal navigation behavior, as the agent prioritizes distance over other critical objectives.\n",
    "\n",
    "---\n",
    "\n",
    "#### Subsection 3: Comparative Analysis and Key Insights\n",
    "\n",
    "- **Simpler Reward Model**:\n",
    "  - The simpler model exhibits a steady increase in episode rewards, progressing from -6000 to near 0 across epochs 35 to 72. While this indicates effective learning, the high variability in rewards suggests that the agent has not yet converged to a stable, optimal policy.\n",
    "  - Without detailed reward component data, specific behavioral improvements (e.g., collision avoidance, lane adherence) are difficult to assess, but the overall trend reflects enhanced navigation capability.\n",
    "\n",
    "- **Older, More Complicated Reward Model**:\n",
    "  - The older model displays significant reward volatility, with occasional high-reward episodes (e.g., 20,000 at episode 10) followed by drops to lower values (e.g., 2,000 at epoch 50). Distance traveled and episode lengths also fluctuate widely.\n",
    "  - The reward components highlight a heavy reliance on \"progress,\" with minimal contributions from safety or efficiency metrics. This skewed reward structure likely undermines consistent performance by favoring forward movement over balanced navigation.\n",
    "\n",
    "**Insight**:\n",
    "The simpler reward model, despite its simplicity, shows more consistent improvement in rewards over time, suggesting that a straightforward, well-tuned reward function may outperform a complex one in this context. Conversely, the older model’s focus on progress at the expense of other behaviors leads to erratic outcomes, underscoring the need for a balanced reward design.\n",
    "\n",
    "---\n",
    "\n",
    "#### Subsection 4: Impact of Reward Function on Agent Behavior\n",
    "\n",
    "- **Simpler Reward Model**:\n",
    "  - The steady reward improvement implies that the agent is learning to balance basic navigation tasks, such as advancing while avoiding obstacles. However, the lack of reward component breakdown limits insight into specific behavioral advancements.\n",
    "\n",
    "- **Older Reward Model**:\n",
    "  - The dominance of the \"progress\" component likely drives the agent to prioritize distance covered over safety or precision. The high distance traveled at epoch 50 (120 meters) despite a modest reward (2,000) suggests risky actions, such as neglecting collisions or lane discipline, which contribute little to the reward.\n",
    "\n",
    "**Observation**:\n",
    "The older model’s reward structure may encourage unsafe navigation by underweighting critical safety and efficiency metrics. This highlights the importance of a reward function that integrates multiple objectives—progress, safety, and adherence—to foster robust autonomous navigation.\n",
    "\n",
    "---\n",
    "\n",
    "#### Subsection 5: Moving to a Simpler Domain: DonkeySim\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Simulation-Only Training** – Our model was trained exclusively in the DonkeySim and CARLA simulation environments, which do not fully replicate real-world conditions such as road irregularities, unpredictable traffic, and weather variations. This limitation restricts the applicability of our trained policies to real-world driving scenarios. Future work should explore domain adaptation techniques to bridge this gap.  \n",
    "\n",
    "- **Reward Function Complexity** – While the reward function evolved throughout development, it still primarily incentivizes forward progress over essential driving behaviors like collision avoidance and lane adherence. This imbalance can lead to suboptimal driving policies that prioritize speed over safety. Further refining the reward function by incorporating multi-objective learning could improve decision-making.  \n",
    "\n",
    "- **Limited Data Diversity** – Our training data was collected on a small number of fixed tracks, limiting the model’s ability to generalize to new and unseen environments. A more diverse dataset, including varying road types and environmental conditions, is necessary to enhance the model’s robustness.  \n",
    "\n",
    "\n",
    "### Future work\n",
    "\n",
    "1. **Bridging the Sim-to-Real Gap** – To enhance real-world applicability, future research should incorporate real-world sensor data and domain adaptation techniques to improve transferability beyond simulations. Training with real-world data or testing the model on physical robotic platforms would provide a clearer understanding of its feasibility.  \n",
    "\n",
    "2. **Enhanced Reward Engineering** – Developing a more balanced reward function that properly accounts for safety, lane adherence, and collision avoidance will help refine the model’s driving behavior. Implementing multi-objective reinforcement learning techniques could further optimize the policy to balance multiple factors effectively.  \n",
    "\n",
    "3. **Hardware Deployment & Real-World Testing** – Deploying the trained model on actual autonomous vehicles or small-scale robots would be crucial for evaluating real-time performance. Future work should focus on testing in physical environments with real-world constraints such as hardware latency, sensor noise, and dynamic traffic conditions. \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "- **Safety & Accountability** – The deployment of RL-driven autonomous vehicles raises critical safety concerns, as errors in decision-making could lead to accidents. Ensuring accountability in case of system failures is essential, requiring stringent safety testing before real-world implementation.  \n",
    "\n",
    "- **Bias & Fairness** – Since the training data is collected in controlled conditions, there is a risk of bias in how the model perceives and reacts to real-world situations. To mitigate this, future models should incorporate diverse training datasets that account for a wide range of environmental conditions and driving scenarios.  \n",
    "\n",
    "- **Explainability & Transparency** – Reinforcement learning models often function as black-box systems, making it difficult to interpret their decision-making processes. Enhancing transparency through explainable AI techniques and interpretability tools would help build trust and ensure accountability in autonomous driving applications. \n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This project explored the application of reinforcement learning for autonomous vehicle navigation in DonkeySim and CARLA. By implementing and evaluating Proximal Policy Optimization (PPO) and Actor-Critic methods, we demonstrated their potential in training self-driving agents within a simulated environment. Our results highlighted the importance of a well-designed reward function for effective learning but also revealed challenges in balancing safety, lane adherence, and progress incentives. Additionally, model performance remained highly sensitive to training conditions, hyperparameter choices, and the diversity of training environments.\n",
    "\n",
    "While RL shows promise in autonomous driving, significant challenges remain in bridging the sim-to-real gap, ensuring safety, and addressing ethical concerns such as bias and transparency. Future work should focus on optimizing learning efficiency, deploying models on real-world robotic platforms, and integrating privacy safeguards to align with regulatory standards. As reinforcement learning continues to evolve, advancements in reward shaping, model-based RL, real-world validation, and ethical AI frameworks will be critical in bringing AI-driven autonomous systems closer to practical deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"kirannote\"></a>[<sup>[1]</sup>](#kiran) Kiran et al. (2021). \"Deep Reinforcement Learning for Autonomous Driving: A Survey.\" A comprehensive review of RL applications in self-driving cars. [Link](https://arxiv.org/pdf/2002.00444.pdf)  \n",
    "\n",
    "<a name=\"pengnote\"></a>[<sup>[2]</sup>](#peng) Peng et al. (2018). \"Sim-to-Real Transfer of Robotic Control with Dynamics Randomization.\" A study on addressing the simulation-to-reality gap in RL applications. [Link](https://arxiv.org/pdf/1710.06537.pdf)\n",
    "\n",
    "<a name=\"taylornote\"></a>[<sup>[3]</sup>](#taylor) Taylor & Stone (2009). \"Transfer Learning for Reinforcement Learning Domains: A Survey.\" A discussion on transfer learning techniques to improve RL generalization. [Link](https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/JMLR09-taylor.pdf)  \n",
    "\n",
    "<a name=\"donkeysimnote\"></a>[<sup>[4]</sup>](#donkeysim) DonkeyCar. Official Documentation. [Link](https://docs.donkeycar.com/guide/deep_learning/simulator/)  \n",
    "\n",
    "<a name=\"carlanote\"></a>[<sup>[5]</sup>](#carla) CARLA Simulator. Official Documentation. [Link](https://carla.readthedocs.io/en/latest/)  \n",
    "\n",
    "<a name=\"pponote\"></a>[<sup>[6]</sup>](#ppo) Schulman et al. (2017). \"Proximal Policy Optimization Algorithms.\" A foundational paper on PPO and its advantages over previous policy gradient methods. [Link](https://arxiv.org/pdf/1707.06347.pdf)  \n",
    "\n",
    "<a name=\"actorcriticnote\"></a>[<sup>[7]</sup>](#actorcritic) Lillicrap et al. (2015). \"Continuous Control with Deep Reinforcement Learning.\" A study on Actor-Critic methods and their effectiveness in continuous control tasks. [Link](https://arxiv.org/pdf/1509.02971.pdf)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
