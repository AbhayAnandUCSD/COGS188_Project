{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **AutoBots: Using Reinforcement Learning with Carla & DonkeySim**\n",
    "\n",
    "### Group members\n",
    "\n",
    "- Abhay Anand\n",
    "- Ashesh Kaji\n",
    "- Varun Pillai\n",
    "- Ansh Bhatnagar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "The goal of this project is to train a Reinforcement Learning (RL) Classifier on autonomous vehicles. We plan to use both CARLA and DonkeyCar Simulator to navigate our vehicle. CARLA will provide a complex urban driving environment, while the DonkeyCar simulator will be used for a simpler track-based navigation on it's “Warren Field” circuit. We plan to solely rely on a Lidar sensor for data collection because of its robustness in capturing depth information and obstacle detection regardless of lighting conditions, an advantage over typical computer vision based data collection. We implement two deep RL algorithms: Actor-Critic and Proximal Policy Optimization (PPO), both designed for continuous action spaces since algorithms like simple a simple Q learning to are ineffective for problems in continious action spaces. We will use the gathered data to train agents to take optimal actions such as steering, acceleration, and braking based on the cars current position relative to the world. Performance will be evaluated using key metrics such as cumulative reward, lap completion time, and distance travelled. By comparing these metrics across different models and training scenarios, we aim to determine which RL method provides the most robust and efficient control for autonomous driving in simulated environments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Autonomous driving has emerged as a rapidly evolving field, as recent advances in computing power\n",
    "and machine learning continue to push the boundaries of vehicle autonomy<a name=\"koutnik\"></a>[<sup>[1]</sup>](#koutniknote).\n",
    "A major breakthrough was the introduction of deep reinforcement learning methods capable of learning\n",
    "complex control policies from high-dimensional data, such as pixel inputs<a name=\"mnih\"></a>[<sup>[2]</sup>](#mnihnote).\n",
    "\n",
    "Among various simulation tools, DonkeyCar Simulator (\"DonkeySim\") provides a relatively accessible\n",
    "environment where researchers can collect training data in a less resource-intensive, hobby-focused\n",
    "setting<a name=\"donkeycar\"></a>[<sup>[3]</sup>](#donkeycarnote). DonkeySim offers a front-facing camera stream,\n",
    "speed readings, and steering angle data—sufficient for exploring end-to-end RL pipelines.\n",
    "\n",
    "Concurrent work in policy optimization techniques, such as Proximal Policy Optimization (PPO),\n",
    "has improved training stability for continuous control tasks, making them more suitable\n",
    "for real-world driving problems<a name=\"schulman\"></a>[<sup>[4]</sup>](#schulmannotenote). By leveraging\n",
    "vision-based RL, robust network architectures, and user-friendly simulators like DonkeySim,\n",
    "researchers aim to accelerate the development of autonomous vehicle control systems.\n",
    "\n",
    "Why is this important? Autonomous driving stands to improve road safety, increase mobility,\n",
    "and reduce congestion. However, it also introduces unique challenges in perception, planning,\n",
    "and control. Studying reinforcement learning in this domain is crucial for advancing algorithms\n",
    "that can handle high-dimensional state spaces and continuous action controls, ultimately bringing\n",
    "us closer to reliable self-driving cars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Autonomous navigation for industrial and factory environments requires precise and efficient vehicle control to ensure safe and timely transportation of goods. Traditional rule-based and vision-based approaches struggle with real-time adaptability and robustness in dynamic settings where numerous unexpected obstacles may arrise due to minor mishaps. Through our project, we aim to develop a deep reinforcement learning (RL) model that enables autonomous vehicles to navigate factory environments using only LiDAR data as input. By leveraging reinforcement learning techniques, particularly Proximal Policy Optimization (PPO) and Actor-Critic methods, we aim to train a model capable of handling continuous action spaces while minimizing computational complexity. Our goal is to create an efficient, collision-free, and fast driving policy that enhances safety, accuracy, and cost-effectiveness in automated logistics and manufacturing operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "For this reinforcement learning project, we generated training data through interactions with the CARLA simulator, a high-fidelity environment designed for autonomous vehicle research. Unlike traditional datasets, our approach relies on real-time sensory inputs from the simulator, with LiDAR data serving as the cornerstone of our state representation.\n",
    "\n",
    "## LiDAR Data Collection for CARLA Implementation\n",
    "- **LiDAR Sensor**: We attached a LiDAR sensor to the vehicle in CARLA, configured with a 50-meter range to capture point cloud data representing distances to surrounding objects.\n",
    "- **Data Processing**: The raw LiDAR point cloud is processed to create a compact yet informative state:\n",
    "  - The point cloud is divided into 8 sectors, each spanning 45 degrees around the vehicle (from -180° to 180°).\n",
    "  - For each sector, we compute the minimum and maximum distances to obstacles, yielding 16 features (8 minimums and 8 maximums). If no points are detected in a sector, a default distance of 50 meters is used.\n",
    "  - This approach reduces computational complexity while preserving spatial awareness, enabling the agent to detect obstacles in all directions.\n",
    "- **State Augmentation**: The LiDAR-derived features are augmented with two additional variables:\n",
    "  - Distance to the next waypoint, calculated as the Euclidean distance from the vehicle’s current position to the target waypoint’s location.\n",
    "  - Angular difference between the vehicle’s current heading (yaw) and the direction to the next waypoint, normalized to [-180°, 180°].\n",
    "  - This results in an 18-dimensional state vector (16 LiDAR features + 2 navigation features).\n",
    "- **Implementation Details**: In our `CarlaEnvWrapper` class, the `_get_state` method processes LiDAR data by converting raw points into polar coordinates (angles and distances), binning them into sectors, and computing min/max distances. The state is updated synchronously with each simulation tick to ensure consistency.\n",
    "\n",
    "By using LiDAR data instead of camera inputs, we avoid the computational overhead of convolutional neural networks, making our model efficient for real-time industrial applications while maintaining robust environmental perception.\n",
    "\n",
    "## DonkeySim Implementation\n",
    "\n",
    "### LiDAR-Based State Representation\n",
    "- Instead of relying on the default **camera feed**, we integrate a **virtual LiDAR sensor** for spatial awareness.\n",
    "- The LiDAR readings are **discretized** into three key sectors: **left, center, and right**, providing depth perception.\n",
    "\n",
    "### Additional State Variables\n",
    "- **Speed of the vehicle**\n",
    "- **Distance to track boundaries** (to prevent off-track behavior)\n",
    "- **Angular deviation from the optimal racing line** (helps maintain smooth turns)\n",
    "\n",
    "### Experience Collection & Training Data\n",
    "- Each interaction produces an experience tuple: **(state, action, reward, next_state)**.\n",
    "- Actions taken: **steering, acceleration, and braking**.\n",
    "- Rewards are assigned based on:\n",
    "  - **Staying on track**\n",
    "  - **Maintaining an optimal speed**\n",
    "  - **Avoiding excessive braking or sharp turns**\n",
    "\n",
    "### Enhancing Generalization\n",
    "- Experimenting with **different reward functions** to balance exploration and exploitation.\n",
    "- Testing across **multiple track variations** for robustness.\n",
    "- Adjusting **penalties for off-track behavior** and **excessive steering** to promote smooth driving.\n",
    "\n",
    "### Efficient RL Training Pipeline\n",
    "- The collected data is used to **iteratively train the PPO and SAC models**.\n",
    "- By refining **reward structures** and **state representations**, we optimize the agent’s learning process for **autonomous driving in DonkeySim**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "### 1. State Representation\n",
    "- The vehicle relies solely on LiDAR data, processed into an 18-dimensional state vector (16 LiDAR features + 2 waypoint features), as detailed in the Data section.\n",
    "- This design ensures a low computational footprint and real-time decision-making capability.\n",
    "\n",
    "### 2. Action Space\n",
    "- The action space is continuous, with two dimensions: throttle/brake ([-1, 1], where positive values are throttle and negative are brake) and steering ([-1, 1]).\n",
    "- This allows the agent to dynamically adjust speed and direction, learning the interplay between velocity and turning for smooth navigation.\n",
    "\n",
    "### 3. Reinforcement Learning Approach\n",
    "- We implemented Proximal Policy Optimization (PPO) using PyTorch Lightning, training the agent in the CARLA simulator.\n",
    "- PPO balances exploration and exploitation, making it ideal for our continuous control task.\n",
    "#### Neural Network Architecture\n",
    "The PPO agent relies on two distinct neural networks: the actor, which determines the policy (action selection), and the critic, which estimates the value function. These networks are designed as multi-layer perceptrons (MLPs) with the following structures:\n",
    "\n",
    "- **Actor Network**:\n",
    "  - **Structure**: A four-layer MLP:\n",
    "    - **Input Layer**: 18 neurons, corresponding to the state dimension (e.g., sensor data, velocity, etc.).\n",
    "    - **Hidden Layers**: Three layers with 256, 256, and 128 neurons, respectively, each followed by Tanh activation functions.\n",
    "    - **Output Layer**: 4 neurons, representing the mean and log standard deviation (log_std) for two continuous actions: throttle and steering (2 neurons per action).\n",
    "  - **Output Processing**:\n",
    "    - **Throttle Mean**: Passed through a sigmoid function to produce values in the range [0,1], biasing the agent toward forward movement.\n",
    "    - **Steering Mean**: Passed through a tanh function to produce values in the range [-1,1], enabling smooth left and right turns.\n",
    "    - **Standard Deviation**: The log_std outputs are exponentiated, clamped, and constrained to a minimum value to ensure sufficient exploration during training.\n",
    "  - **Initialization**: Weights are initialized using Xavier uniform initialization, and biases are set to zero.\n",
    "  - **Purpose**: The Tanh activations help stabilize policy updates, while the split output design accommodates the continuous action space required for driving control.\n",
    "\n",
    "- **Critic Network**:\n",
    "  - **Structure**: A four-layer MLP:\n",
    "    - **Input Layer**: 18 neurons, matching the state dimension.\n",
    "    - **Hidden Layers**: Three layers with 256, 256, and 128 neurons, respectively, each followed by ReLU activation functions.\n",
    "    - **Output Layer**: 1 neuron, providing the value estimate for the given state.\n",
    "  - **Initialization**: Weights are initialized using Xavier uniform initialization, and biases are set to zero.\n",
    "  - **Purpose**: The ReLU activations support effective value approximation, enabling the critic to provide stable and accurate estimates of the state’s expected return.\n",
    "\n",
    "### 4. Reward Function Design\n",
    "The reward function evolved iteratively to guide the agent toward safe, efficient, and route-following behavior:\n",
    "\n",
    "- **Initial Reward Function**:\n",
    "  - **Collision Avoidance**: A penalty of -50 was applied for collisions to prioritize safety.\n",
    "  - **Speed Maintenance**: Reward was proportional to distance traveled per step, encouraging forward movement.\n",
    "  - This basic design promoted movement while avoiding obstacles but lacked route guidance.\n",
    "\n",
    "- **Intermediate Reward Function**:\n",
    "  - **Lane Discipline**: Added a -1 penalty for lane invasions to keep the vehicle within track boundaries.\n",
    "  - **Speed Regulation**: Introduced a target speed of 30 km/h, with a penalty (-0.1 * |speed - target|) for deviations, and an additional -1 penalty for speeds below 5 km/h to prevent stalling.\n",
    "  - **Steering Smoothness**: Penalized large steering actions (-0.5 * |steering|) when speed was below 5 km/h to reduce erratic behavior at low speeds.\n",
    "  - This improved track adherence and consistency but didn’t ensure progress along a specific path.\n",
    "\n",
    "- **Final Reward Function**:\n",
    "  - **Waypoint Following**: Added a reward based on proximity to the next waypoint (max(0, 5 - distance/10)), encouraging route adherence.\n",
    "  - **Heading Alignment**: Included a bonus (max(0, 1 - |angle_diff|/180)) for aligning the vehicle’s heading with the waypoint direction, promoting smoother turns.\n",
    "  - **Progress Reward**: Retained distance traveled as a base reward, augmented by waypoint incentives.\n",
    "  - **Safety Penalties**: Kept collision (-50) and lane invasion (-1) penalties.\n",
    "  - **Stuck Detection**: Penalized (-2) if the vehicle’s position varied by less than 1 meter over 20 steps, preventing circular or stagnant behavior.\n",
    "  - Implemented in `CarlaEnvWrapper.step`, this final version balances safety, efficiency, and navigation.\n",
    "\n",
    "### 5. Deployment and Applications\n",
    "#### Setup deployment details\n",
    "A full list complete setup details can be found in the code repo's README. Essentially the following items must be setup:\n",
    "1. Carla 0.9.15 must be setup on a gpu based machine, as well as the python api for the same version for full compatibility\n",
    "2. Python must be setup with respective packages\n",
    "3. The car is setup in the default map with the default settings. It is a real world simulation, but with no NPCs to reduce complexity given the projects scale.\n",
    "#### Potential Future applications\n",
    "- The trained PPO model can optimize logistics in factory settings, enabling autonomous vehicles to transport goods safely and efficiently along predefined routes.\n",
    "- This approach reduces costs and enhances precision in industrial automation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "#### 1. Cumulative Reward\n",
    "- **Definition**: The total reward accumulated over an episode, calculated based on the reward function defined in your project.\n",
    "- **Significance**: This metric reflects the overall performance of the agent. Higher cumulative rewards indicate better navigation, fewer collisions, and more effective adherence to the intended route. It serves as a primary indicator of policy improvement during training.\n",
    "\n",
    "#### 2. Collision Rate\n",
    "- **Definition**: The frequency of collisions with obstacles or boundaries during an episode.\n",
    "- **Significance**: A lower collision rate is desirable, as it demonstrates the agent’s ability to navigate safely and avoid hazards. This metric is critical for evaluating the safety performance of the driving policy.\n",
    "\n",
    "#### 3. Distance Traveled\n",
    "- **Definition**: The total distance covered by the vehicle over the course of an episode.\n",
    "- **Significance**: When paired with a low collision rate, a higher distance traveled suggests efficient and effective navigation. This metric highlights the agent’s progress and ability to follow the desired path.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Reference the videos here: https://drive.google.com/drive/folders/1K7cDpq456Woh-fQq7A6EAiD3Nz-YboTQ?usp=sharing\n",
    "\n",
    "\n",
    "The primary objective of this study is to demonstrate that a well-designed reward function is essential for effective reinforcement learning (RL)-based autonomous navigation. Additional considerations include the utility of LiDAR data for state representation and the appropriateness of PPO for continuous control tasks. We evaluate the PPO agent's performance using two distinct reward models: a simpler reward-based model and an older, more complicated reward-based model. The analysis centers on key performance metrics, including episode rewards, distance traveled, episode lengths, and reward components, derived from the training progress at epoch 50 for the older model and the rewards vs. steps relationship for the simpler model.\n",
    "\n",
    "---\n",
    "#### Subsection 1: Performance of the Simpler Reward-Based Model\n",
    "<img src=\"Carla_PPO_agent/training_progress_epoch_50.png\" alt=\"Training Progress Epoch 50\" />\n",
    "The performance of the PPO agent with the simpler reward-based model is illustrated in a scatter plot titled \"Episode Reward vs Steps (Colored by Epoch),\" which tracks episode rewards against the number of steps across epochs 35 to 72.\n",
    "\n",
    "- **Early Epochs (35-40)**:\n",
    "  - During the initial training phase (epochs 35-40), episode rewards are consistently low, ranging between -6000 and -4000. At around 100 steps, rewards frequently reach -6000, reflecting poor performance early in training. As steps increase to 200, rewards remain predominantly negative, with significant variability—some points improve to -1000, while others drop back to -5000 or lower.\n",
    "\n",
    "- **Mid Epochs (41-55)**:\n",
    "  - In the mid-training phase (epochs 41-55), the agent's performance begins to improve. Between 200 and 300 steps, episode rewards cluster between -4000 and -2000, with fewer points falling below -5000. Variability persists, with some episodes achieving rewards as high as -1000, while others remain around -4000, indicating inconsistent progress.\n",
    "\n",
    "- **Later Epochs (56-72)**:\n",
    "  - By the later epochs (56-72), further improvement is evident. Between 300 and 450 steps, rewards concentrate between -3000 and -1000, with several points nearing 0 (e.g., around -500 at 400-450 steps). However, some outliers still drop to -4000, suggesting that the agent has not yet achieved fully stable performance.\n",
    "\n",
    "**Key Observation**:\n",
    "The scatter plot reveals a clear upward trend in episode rewards as both steps and epochs increase, demonstrating that the PPO agent is learning and refining its navigation abilities under the simpler reward model. Nevertheless, the persistent variability in rewards indicates that additional training or model refinement may be required to achieve consistent and optimal performance.\n",
    "\n",
    "---\n",
    "\n",
    "#### Subsection 2: Performance of the Older, More Complicated Reward-Based Model (Training Progress at Epoch 50)\n",
    "<img src=\"Carla_PPO_agent/reward_vs_steps_by_epoch.png\" alt=\"Training Progress Epoch 50\" />\n",
    "The older, more complicated reward-based model’s performance is assessed using four charts depicting training progress over 100 epochs. Here, we focus specifically on the agent's behavior at epoch 50.\n",
    "\n",
    "- **Episode Rewards**:\n",
    "  - At epoch 50, the total reward is approximately 2,000. This follows a peak of around 10,000 at episode 40 and a subsequent decline. Across the 100 epochs, rewards fluctuate widely, with notable spikes at episode 10 (around 20,000), episode 60 (around 15,000), and episode 80 (around 5,000), but they generally hover around 2,000 outside these peaks.\n",
    "\n",
    "- **Distance Traveled**:\n",
    "  - The distance traveled at epoch 50 reaches approximately 120 meters, a significant peak compared to surrounding episodes, which typically range between 20 and 40 meters. Other prominent peaks include episode 10 (120 meters), episode 30 (100 meters), episode 70 (80 meters), and episode 90 (140 meters).\n",
    "\n",
    "- **Episode Lengths**:\n",
    "  - At epoch 50, the episode length is around 300 steps, relatively modest compared to earlier peaks like episode 10 (900 steps). Episode lengths generally fluctuate between 200 and 400 steps, with occasional spikes, such as 600 steps at episode 60 and 700 steps at episode 80.\n",
    "\n",
    "- **Reward Components**:\n",
    "  - The total reward at epoch 50 is dominated by the \"progress\" component (approximately 2,000), with a minor contribution from the \"longevity\" component (around 500). Other components—collision, lane invasion, lane centering, heading, speed, steering smoothness, survival bonus, and completion bonus—remain near zero, indicating negligible influence on the total reward.\n",
    "\n",
    "**Key Observation**:\n",
    "At epoch 50, the older model yields a total reward of 2,000, with a notable distance traveled of 120 meters. The overwhelming contribution of the \"progress\" reward component suggests that the agent is primarily incentivized to move forward, while safety and efficiency metrics (e.g., collision avoidance, lane discipline) play a minimal role. This imbalance may result in suboptimal navigation behavior, as the agent prioritizes distance over other critical objectives.\n",
    "\n",
    "---\n",
    "\n",
    "#### Subsection 3: Comparative Analysis and Key Insights\n",
    "\n",
    "- **Simpler Reward Model**:\n",
    "  - The simpler model exhibits a steady increase in episode rewards, progressing from -6000 to near 0 across epochs 35 to 72. While this indicates effective learning, the high variability in rewards suggests that the agent has not yet converged to a stable, optimal policy.\n",
    "  - Without detailed reward component data, specific behavioral improvements (e.g., collision avoidance, lane adherence) are difficult to assess, but the overall trend reflects enhanced navigation capability.\n",
    "\n",
    "- **Older, More Complicated Reward Model**:\n",
    "  - The older model displays significant reward volatility, with occasional high-reward episodes (e.g., 20,000 at episode 10) followed by drops to lower values (e.g., 2,000 at epoch 50). Distance traveled and episode lengths also fluctuate widely.\n",
    "  - The reward components highlight a heavy reliance on \"progress,\" with minimal contributions from safety or efficiency metrics. This skewed reward structure likely undermines consistent performance by favoring forward movement over balanced navigation.\n",
    "\n",
    "**Insight**:\n",
    "The simpler reward model, despite its simplicity, shows more consistent improvement in rewards over time, suggesting that a straightforward, well-tuned reward function may outperform a complex one in this context. Conversely, the older model’s focus on progress at the expense of other behaviors leads to erratic outcomes, underscoring the need for a balanced reward design.\n",
    "\n",
    "---\n",
    "\n",
    "#### Subsection 4: Impact of Reward Function on Agent Behavior\n",
    "\n",
    "- **Simpler Reward Model**:\n",
    "  - The steady reward improvement implies that the agent is learning to balance basic navigation tasks, such as advancing while avoiding obstacles. However, the lack of reward component breakdown limits insight into specific behavioral advancements.\n",
    "\n",
    "- **Older Reward Model**:\n",
    "  - The dominance of the \"progress\" component likely drives the agent to prioritize distance covered over safety or precision. The high distance traveled at epoch 50 (120 meters) despite a modest reward (2,000) suggests risky actions, such as neglecting collisions or lane discipline, which contribute little to the reward.\n",
    "\n",
    "**Observation**:\n",
    "The older model’s reward structure may encourage unsafe navigation by underweighting critical safety and efficiency metrics. This highlights the importance of a reward function that integrates multiple objectives—progress, safety, and adherence—to foster robust autonomous navigation.\n",
    "\n",
    "---\n",
    "\n",
    "#### Subsection 5: Moving to a Simpler Domain: DonkeySim\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Simulation-Only Training** – Our model was trained exclusively in the DonkeySim and CARLA simulation environments, which do not fully replicate real-world conditions such as road irregularities, unpredictable traffic, and weather variations. This limitation restricts the applicability of our trained policies to real-world driving scenarios. Future work should explore domain adaptation techniques to bridge this gap.  \n",
    "\n",
    "- **Reward Function Complexity** – While the reward function evolved throughout development, it still primarily incentivizes forward progress over essential driving behaviors like collision avoidance and lane adherence. This imbalance can lead to suboptimal driving policies that prioritize speed over safety. Further refining the reward function by incorporating multi-objective learning could improve decision-making.  \n",
    "\n",
    "- **Limited Data Diversity** – Our training data was collected on a small number of fixed tracks, limiting the model’s ability to generalize to new and unseen environments. A more diverse dataset, including varying road types and environmental conditions, is necessary to enhance the model’s robustness.  \n",
    "\n",
    "\n",
    "### Future work\n",
    "\n",
    "1. **Bridging the Sim-to-Real Gap** – To enhance real-world applicability, future research should incorporate real-world sensor data and domain adaptation techniques to improve transferability beyond simulations. Training with real-world data or testing the model on physical robotic platforms would provide a clearer understanding of its feasibility.  \n",
    "\n",
    "2. **Enhanced Reward Engineering** – Developing a more balanced reward function that properly accounts for safety, lane adherence, and collision avoidance will help refine the model’s driving behavior. Implementing multi-objective reinforcement learning techniques could further optimize the policy to balance multiple factors effectively.  \n",
    "\n",
    "3. **Hardware Deployment & Real-World Testing** – Deploying the trained model on actual autonomous vehicles or small-scale robots would be crucial for evaluating real-time performance. Future work should focus on testing in physical environments with real-world constraints such as hardware latency, sensor noise, and dynamic traffic conditions. \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "- **Safety & Accountability** – The deployment of RL-driven autonomous vehicles raises critical safety concerns, as errors in decision-making could lead to accidents. Ensuring accountability in case of system failures is essential, requiring stringent safety testing before real-world implementation.  \n",
    "\n",
    "- **Bias & Fairness** – Since the training data is collected in controlled conditions, there is a risk of bias in how the model perceives and reacts to real-world situations. To mitigate this, future models should incorporate diverse training datasets that account for a wide range of environmental conditions and driving scenarios.  \n",
    "\n",
    "- **Explainability & Transparency** – Reinforcement learning models often function as black-box systems, making it difficult to interpret their decision-making processes. Enhancing transparency through explainable AI techniques and interpretability tools would help build trust and ensure accountability in autonomous driving applications. \n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This project explored the application of reinforcement learning for autonomous vehicle navigation in DonkeySim and CARLA. By implementing and evaluating Proximal Policy Optimization (PPO) and Actor-Critic methods, we demonstrated their potential in training self-driving agents within a simulated environment. Our results highlighted the importance of a well-designed reward function for effective learning but also revealed challenges in balancing safety, lane adherence, and progress incentives. Additionally, model performance remained highly sensitive to training conditions, hyperparameter choices, and the diversity of training environments.\n",
    "\n",
    "While RL shows promise in autonomous driving, significant challenges remain in bridging the sim-to-real gap, ensuring safety, and addressing ethical concerns such as bias and transparency. Future work should focus on optimizing learning efficiency, deploying models on real-world robotic platforms, and integrating privacy safeguards to align with regulatory standards. As reinforcement learning continues to evolve, advancements in reward shaping, model-based RL, real-world validation, and ethical AI frameworks will be critical in bringing AI-driven autonomous systems closer to practical deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
