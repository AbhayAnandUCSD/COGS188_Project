{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 181 - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert title here\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Abhay Anand\n",
    "- Ashesh Kaji\n",
    "- Varun Pillai\n",
    "- Ansh Bhatnagar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "The goal of this project is to train a Reinforcement Learning (RL) Classifier on autonomous vehicles. We plan to use DonkeyCar Simulator to navigate the \"Warren Feild\" track and collect our data. The data we plan to collect includes visual input from the front-facing camera, speed and steering angle change. These will be collected during training in the simulated environment. We implement and compare two deep RL algorithms: Actor-Critic and Proximal Policy Optimization (PPO), both designed for continuous action spaces. Moreover we plan to include a simple Q learning model to show how inefficient/ ineffective it is for more complex problems. We will use the gathered data to train agents to take optimal actions such as steering left/right, acceleration, and braking based on the cars current position. Performance will be evaluated using key metrics such as cumulative reward, lap completion time, and DonkeySim's own accuracy rating. By comparing these metrics across different models and training scenarios, we aim to determine which RL method provides the most robust and efficient control for autonomous driving in simulated environments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Autonomous driving has emerged as a rapidly evolving field, as recent advances in computing power\n",
    "and machine learning continue to push the boundaries of vehicle autonomy<a name=\"koutnik\"></a>[<sup>[1]</sup>](#koutniknote).\n",
    "A major breakthrough was the introduction of deep reinforcement learning methods capable of learning\n",
    "complex control policies from high-dimensional data, such as pixel inputs<a name=\"mnih\"></a>[<sup>[2]</sup>](#mnihnote).\n",
    "\n",
    "Among various simulation tools, DonkeyCar Simulator (\"DonkeySim\") provides a relatively accessible\n",
    "environment where researchers can collect training data in a less resource-intensive, hobby-focused\n",
    "setting<a name=\"donkeycar\"></a>[<sup>[3]</sup>](#donkeycarnote). DonkeySim offers a front-facing camera stream,\n",
    "speed readings, and steering angle data—sufficient for exploring end-to-end RL pipelines.\n",
    "\n",
    "Concurrent work in policy optimization techniques, such as Proximal Policy Optimization (PPO),\n",
    "has improved training stability for continuous control tasks, making them more suitable\n",
    "for real-world driving problems<a name=\"schulman\"></a>[<sup>[4]</sup>](#schulmannotenote). By leveraging\n",
    "vision-based RL, robust network architectures, and user-friendly simulators like DonkeySim,\n",
    "researchers aim to accelerate the development of autonomous vehicle control systems.\n",
    "\n",
    "Why is this important? Autonomous driving stands to improve road safety, increase mobility,\n",
    "and reduce congestion. However, it also introduces unique challenges in perception, planning,\n",
    "and control. Studying reinforcement learning in this domain is crucial for advancing algorithms\n",
    "that can handle high-dimensional state spaces and continuous action controls, ultimately bringing\n",
    "us closer to reliable self-driving cars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Autonomous navigation for industrial and factory environments requires precise and efficient vehicle control to ensure safe and timely transportation of goods. Traditional rule-based and vision-based approaches struggle with real-time adaptability and robustness in dynamic settings where numerous unexpected obstacles may arrise due to minor mishaps. Through our project, we aim to develop a deep reinforcement learning (RL) model that enables autonomous vehicles to navigate factory environments using only LiDAR data as input. By leveraging reinforcement learning techniques, particularly Proximal Policy Optimization (PPO) and Actor-Critic methods, we aim to train a model capable of handling continuous action spaces while minimizing computational complexity. Our goal is to create an efficient, collision-free, and fast driving policy that enhances safety, accuracy, and cost-effectiveness in automated logistics and manufacturing operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "For this reinforcement learning project, we will not use a pre-existing dataset to train our agent but we will instead rely on generating training data using the DonkeySim environment, which is an application with pre-made racing tracks for testing autonomous vehicles. The simulator software itself provides essential sensory inputs, including a front-facing camera stream, speed readings, obstacle-hit counters, and steering angle data, which will serve as the basis for our state space for our models (We may tinker with this and not use all of the listed data sources, or we may even add more. Since this is our first time using DonkeySim, we will have to experiment as we go a little bit). \n",
    "\n",
    "The agent will interact with the environment by taking actions such as steering, accelerating, and braking, and it will receive rewards based on reward function's pre-defined criteria, such as staying on track and minimizing lap time. During training, we currently plan to collect and store experience tuples (state, action, reward, next-state) to teach the agent to optimize the learning process. To enhance performance, we may experiment with different reward functions and data augmentation techniques, such as varying tracks, messing around with the weights for the reward function, and penaliing sharp turns to improve model generalization. By training our agent iteratively within DonkeySim, we ensure that our approach remains scalable and adaptable without the need for an external dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "## 1. State Representation  \n",
    "- The vehicle will only use LiDAR data as input to perceive the environment, avoiding the complexity of processing camera images via CNNs.  \n",
    "- This allows for a lower computational footprint while still maintaining robust environmental awareness.  \n",
    "\n",
    "## 2. Action Space  \n",
    "- The model will operate in a continuous action space, controlling both steering and throttle dynamically.  \n",
    "- This ensures that the vehicle can learn the relationship between speed and turning angles, crucial for smooth navigation, all while avoiding obstacles in its path.  \n",
    "\n",
    "## 3. Reinforcement Learning Approach  \n",
    "- The model will be trained using Proximal Policy Optimization (PPO) and Actor-Critic methods to balance exploration and exploitation effectively.  \n",
    "- These methods will be evaluated on a virtual track using CARLA, a high-fidelity autonomous vehicle simulator.  \n",
    "\n",
    "## 4. Reward Function Design  \n",
    "- +1 for progress along the route to encourage efficient movement.  \n",
    "- -10 for collisions to penalize unsafe behavior.  \n",
    "- Additional penalty (-1 scaled) for lane deviation, ensuring the vehicle stays within the designated track boundaries.  \n",
    "\n",
    "## 5. Deployment and Applications  \n",
    "- The trained model will be used to optimize logistics in factory environments, where autonomous trucks can transport goods safely and efficiently between different locations.  \n",
    "- This system will reduce operational costs, enhance safety, and increase delivery precision in industrial settings.  \n",
    "\n",
    "By training in a simulated environment and optimizing for real-world applications, our approach ensures efficient deployment in smart factories and industrial automation, paving the way for scalable, AI-driven logistics solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "## 1. Episode Reward (Cumulative Return)\n",
    "\n",
    "### Definition  \n",
    "The cumulative sum of rewards received over an episode. This is a direct indicator of how well the vehicle is navigating based on the reward function designed.\n",
    "\n",
    "- Higher cumulative rewards indicate better policy learning (efficient navigation, fewer collisions).\n",
    "- Helps compare different RL models (PPO vs. Actor-Critic).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Custom Evaluation Metric: Autonomous Driving Performance Score (ADPS)\n",
    "\n",
    "### Definition  \n",
    "The Autonomous Driving Performance Score (ADPS) is a composite evaluation metric designed to quantify the performance of an autonomous vehicle by considering lap time efficiency, collision rate, and lane deviation. The ADPS score is computed as a weighted sum of these factors and provides a normalized score between 0 and 1, where:\n",
    "\n",
    "- 1 indicates perfect driving performance (fastest lap, no collisions, and staying within lane boundaries).\n",
    "- 0 indicates poor driving performance (slow lap, frequent collisions, and significant lane deviation).\n",
    "\n",
    "ADPS ensures a balance between efficiency, safety, and accuracy, making it a robust metric for evaluating RL-trained autonomous vehicle policies.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Probably you should include a learning curve to demonstrate how much better the model gets as you increase the number of trials\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Generally reinforement learning tasks may require a huge amount of training, so extensive grid search is unlikely to be possible. However expoloring a few reasonable hyper-parameters may still be possible.  Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?  Or you compare a completely different approach/alogirhtm to the problem? Whatever, this stuff is just serving suggestions.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "\n",
    "### Future work\n",
    "Looking at the limitations and/or the toughest parts of the problem and/or the situations where the algorithm(s) did the worst... is there something you'd like to try to make these better.\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination.\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
